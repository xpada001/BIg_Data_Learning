{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install mongoDB in Ambari\n",
    "```\n",
    "cd /var/lib/ambari-server/resources/stacks/HDP/2.5/services\n",
    "git clone https://github.com/nikunjness/mongo-ambari.git\n",
    "sudo service ambari restart\n",
    "```\n",
    "Then log in to Ambari and 'add service' -> mongoDB\n",
    "\n",
    "Make sure to install pymongo in ambari server as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def parseInput(line):\n",
    "    fields = line.split('|')\n",
    "    return Row(user_id = int(fields[0]), age = int(fields[1]), gender = fields[2], occupation = fields[3], zip = fields[4]) #row name needs to match with Cassandra table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder.appName(\"MongoDBIntegration\").getOrCreate()\n",
    "\n",
    "    # Get the raw data\n",
    "    lines = spark.sparkContext.textFile(\"hdfs://127.0.0.1:8020/user/maria_dev/ml-100k/u.data\")\n",
    "    # Convert it to a RDD of Row objects with (userID, age, gender, occupation, zip)\n",
    "    users = lines.map(parseInput)\n",
    "    # Convert that to a DataFrame\n",
    "    usersDataset = spark.createDataFrame(users)\n",
    "\n",
    "    # Write it into Mongo\n",
    "    usersDataset.write\\\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .options(\"uri\",\"mongodb://127.0.0.1/movielens.users\")\\\n",
    "        .mode('append')\\\n",
    "        .save()\n",
    "\n",
    "    # Read it back from Mongo into a new Dataframe\n",
    "    readUsers = spark.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .options(\"uri\",\"mongodb://127.0.0.1/movielens.users\")\\\n",
    "    .load() #this will not load the data into memory\n",
    "\n",
    "    readUsers.createOrReplaceTempView(\"users\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM users WHERE age < 20\")\n",
    "    sqlDF.show() #here what this is doing is figuring out how to translate this query into mongoDB language, then it will query in mongo\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mongo Shell\n",
    "\n",
    "```\n",
    "mongo\n",
    "use movielens\n",
    "db.users.find({user_id:100}) to find the record with user_id 100\n",
    "```\n",
    "\n",
    "the find above is not too efficient as it is not index and it is scanning the entire database. We can create an index\n",
    "\n",
    "```\n",
    "db.users.createIndex({user_id:1}) #1 means ascending\n",
    "```\n",
    "other mongo commands\n",
    "```\n",
    "db.users.aggregate([\n",
    "    { $group: { _id: {occupation: \"$occupation\"}, avgAge: { $avg: \"$age\"}}}\n",
    "    ])\n",
    "    \n",
    "db.users.count()\n",
    "db.getCollectionInfos()\n",
    "db.users.drop()\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
