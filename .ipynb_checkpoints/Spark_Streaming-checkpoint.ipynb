{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to consume real time data and count how many times each unique url in the web logs appears in a timeframe (i.e. windowed interval) of 5 minutes. \n",
    "\n",
    "Get flume talk to spark streaming: use the following config file to connect to avro which is a communciation protocol that will later connect to spark streaming. Everything is same as prev code of flume, except the sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkstreamingflume.conf: A single-node Flume configuration\n",
    "\n",
    "# Name the components on this agent\n",
    "a1.sources = r1\n",
    "a1.sinks = k1\n",
    "a1.channels = c1\n",
    "\n",
    "# Describe/configure the source\n",
    "a1.sources.r1.type = spooldir\n",
    "a1.sources.r1.spoolDir = /home/maria_dev/spool\n",
    "a1.sources.r1.fileHeader = true\n",
    "a1.sources.r1.interceptors = timestampInterceptor\n",
    "a1.sources.r1.interceptors.timestampInterceptor.type = timestamp\n",
    "\n",
    "# Describe the sink\n",
    "a1.sinks.k1.type = avro\n",
    "a1.sinks.k1.hostname = localhost\n",
    "a1.sinks.k1.port = 9092\n",
    "\n",
    "# Use a channel which buffers events in memory\n",
    "a1.channels.c1.type = memory\n",
    "a1.channels.c1.capacity = 1000\n",
    "a1.channels.c1.transactionCapacity = 100\n",
    "\n",
    "# Bind the source and sink to the channel\n",
    "a1.sources.r1.channels = c1\n",
    "a1.sinks.k1.channel = c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the spark script SparkFlume.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.flume import FlumeUtils\n",
    "\n",
    "parts = [\n",
    "    r'(?P<host>\\S+)',                   # host %h\n",
    "    r'\\S+',                             # indent %l (unused)\n",
    "    r'(?P<user>\\S+)',                   # user %u\n",
    "    r'\\[(?P<time>.+)\\]',                # time %t\n",
    "    r'\"(?P<request>.+)\"',               # request \"%r\"\n",
    "    r'(?P<status>[0-9]+)',              # status %>s\n",
    "    r'(?P<size>\\S+)',                   # size %b (careful, can be '-')\n",
    "    r'\"(?P<referer>.*)\"',               # referer \"%{Referer}i\"\n",
    "    r'\"(?P<agent>.*)\"',                 # user agent \"%{User-agent}i\"\n",
    "]\n",
    "pattern = re.compile(r'\\s+'.join(parts)+r'\\s*\\Z')\n",
    "\n",
    "def extractURLRequest(line):\n",
    "    exp = pattern.match(line)\n",
    "    if exp:\n",
    "        request = exp.groupdict()[\"request\"]\n",
    "        if request:\n",
    "           requestFields = request.split()\n",
    "           if (len(requestFields) > 1):\n",
    "                return requestFields[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext(appName=\"StreamingFlumeLogAggregator\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    ssc = StreamingContext(sc, 1) # batch interval of 1 second\n",
    "\n",
    "    flumeStream = FlumeUtils.createStream(ssc, \"localhost\", 9092) # this is a push model\n",
    "\n",
    "    lines = flumeStream.map(lambda x: x[1])\n",
    "    urls = lines.map(extractURLRequest)\n",
    "\n",
    "    # Reduce by URL over a 5-minute window sliding every second\n",
    "    urlCounts = urls.map(lambda x: (x, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y : x - y, 300, 1)\n",
    "    # the windowed interval is 5 minutes and slide interval is 1 sec\n",
    "\n",
    "    # Sort and print the results\n",
    "    sortedResults = urlCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "    sortedResults.pprint()\n",
    "\n",
    "    ssc.checkpoint(\"/home/maria_dev/checkpoint\") # a back up location where if failure, can pick it up from here\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the spark script:\n",
    "```\n",
    "spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py\n",
    "```\n",
    "Then, lets kick off flume:\n",
    "```\n",
    "bin/flume-ng agent --conf conf --conf-file ~/sparkstreamingflume.conf --name a1\n",
    "```\n",
    "then you can put some log file in spool folder and test it out, for example you can get it from\n",
    "```\n",
    "wget http://media.sundog-soft.com/hadoop/access_log.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
